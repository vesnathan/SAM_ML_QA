{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOdBrUdQz4MJ"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrPLjHRq-SjE"
      },
      "outputs": [],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFr9OavL-UiX"
      },
      "outputs": [],
      "source": [
        "pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_so-k1mV-X0j"
      },
      "outputs": [],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZoDNT1U-aCo"
      },
      "outputs": [],
      "source": [
        "pip install google.colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSd1iP3H59CY"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AmzreW0y78Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "data = pd.read_csv('/content/data.csv')\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.3, random_state=42)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    # Remove leading/trailing whitespaces\n",
        "    text = text.strip()\n",
        "\n",
        "    # Collapse multiple spaces into a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Define a custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, questions, document_text, answers, labels, tokenizer, max_length):\n",
        "        self.questions = questions\n",
        "        self.document_text = document_text\n",
        "        self.answers = answers\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        question = self.questions[index]\n",
        "        answer = self.answers[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        question = preprocess_text(question)\n",
        "        answer = preprocess_text(answer)\n",
        "        document_text = preprocess_text(self.document_text[index])\n",
        "\n",
        "        input_text = f\"{question} {document_text} {answer}\"\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 16\n",
        "max_length = 512\n",
        "num_epochs = 50\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Load the pre-trained tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Example training data\n",
        "train_questions = train_data['question'].values\n",
        "train_document_text = train_data['document_text'].values\n",
        "train_answers = train_data['answer'].values\n",
        "train_labels = train_data['label'].values\n",
        "\n",
        "# Example validation data\n",
        "val_questions = val_data['question'].values\n",
        "val_document_text = val_data['document_text'].values\n",
        "val_answers = val_data['answer'].values\n",
        "val_labels = val_data['label'].values\n",
        "\n",
        "# Create the custom dataset and data loader for training\n",
        "train_dataset = CustomDataset(train_questions, train_document_text, train_answers, train_labels, tokenizer, max_length)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Create the custom dataset and data loader for validation\n",
        "val_dataset = CustomDataset(val_questions, val_document_text, val_answers, val_labels, tokenizer, max_length)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
        "model.to(device=device)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_preds = []\n",
        "    train_targets = []\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device=device)\n",
        "        attention_mask = batch['attention_mask'].to(device=device)\n",
        "        labels = batch['label'].to(device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_preds.extend(torch.argmax(logits, dim=1).tolist())\n",
        "        train_targets.extend(labels.tolist())\n",
        "\n",
        "        print(f\"Train Batch: Loss={loss.item()}\")\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = accuracy_score(train_targets, train_preds)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_preds = []\n",
        "    val_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device=device)\n",
        "            attention_mask = batch['attention_mask'].to(device=device)\n",
        "            labels = batch['label'].to(device=device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_preds.extend(torch.argmax(logits, dim=1).tolist())\n",
        "            val_targets.extend(labels.tolist())\n",
        "\n",
        "            print(f\"Validation Batch: Loss={loss.item()}\")\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = accuracy_score(val_targets, val_preds)\n",
        "\n",
        "    # Print training and validation loss and accuracy\n",
        "    print(f\"Epoch {epoch+1}:\")\n",
        "    print(f\"Train Loss: {train_loss / len(train_loader)}\")\n",
        "    print(f\"Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"Validation Loss: {val_loss / len(val_loader)}\")\n",
        "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "# Save the trained model\n",
        "model.save_pretrained('./')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
